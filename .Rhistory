#Computing the variance of the sample means
SamVarDistn <- sapply(SamMeanDistn, function(x)(x-(1/lambda))^2)
SamVarDistn
SamVar <- sum(SamVarDistn)/(nosim)
#Theoretical values of mean and variance
IdealMean <- (1/lambda)
IdealVar <- 1/(n*(lambda)^2)
#computing coverage for the population mean with the formula given
temp0 <-((1.96)*sqrt(SamVarDistn/n))
lower = SamMean - temp0
upper = SamMean + temp0
temp1 <- (lower < IdealMean)
temp2 <- (upper > IdealMean)
coverage = mean(temp1&temp2)*100
coverage
#Plot the histogram
hist(SamMeanDistn, freq = FALSE, breaks =25, col = "grey", xlab = "Sample Mean", ylab = "density",
main = ("Histogram of Sample Mean of 40 exponential(0.2)s"))
abline(v = SamMean, col = "blue", lwd = 2)
abline(v = IdealMean, col = "red", lwd = 2)
curve(dnorm(x, mean = IdealMean, sd = IdealMean/sqrt(n)), col = "darkgreen", lwd=2, add = T)
legend("topright", c("Sample Mean", "Population Mean", "Normal Curve"), fill = c("blue", "red", "darkgreen"))
legend("right", legend = c(paste("No. Simulns =", round(nosim, 1)),
paste("Theor. Var. =",round(IdealVar, 1)),
paste("Act. Var. =",round(SamVar, 1)))
, bty = "n")
#Return values of differences between theoretical and actual mean and variance.
Result <- c(abs(IdealMean-SamMean), abs(IdealVar-SamVar), coverage)
Result
}
# R-code for simulating 1,000 sample means where each sample mean is comprised of 40 observations.
n =40
nosim = 1000
result <- MeanVarForSam( n, nosim)
humanReadable <- c(paste0("the differenc between theoretical and actual sample means is ", as.character(result[1])),
paste0("the differenc between theoretical and actual variance of the sample distribution is ", as.character(result[2])),
paste0("The percentage coverage for 1/lambda is ", as.character(result[3])))
humanReadable
Simulation and Inferential Data Analysis
=======================================
One important lesson in statistics is to understand how sample means behave. Often times one of the things people do with statistics is compare populations, and most of the time populations are compared by comparing their means. This is a good motivation to understand the behavior of
sample means and what we can infer about the population mean. We conduct this study using simulated data.
The code below simulates 1000 sample means, each sample consisting of 40
random exponentially distributed values with 'lambda' = 0.2 and a histogram
of the output is plotted. The following numbers are computed:
1. The average of the sample means which is **the value at which the sample means are
centered**. It is shown as the blue vertical line in the figure.
2. The theoretical center of the distribution, shown as a red vertical line in the histogram.
3. The theoretical value of variance of the sample means, which is given by (/n*(lambda)^2), displayed in
the histogram as a text.
4. The actual value of the variance of the sample means, also shonw as text in the histogram.
5. In addition, the normal distribution is overlaid on the histogram (line in green) to demonstrate
the fact that the sample means are distributed approximately normally.
6. with the formula given to us, coverage is computed.
We wrote a function called MeanVarForSam(n, nosim, Plot, Result), where n is the sample size, nosim is the number of simulations, and 'Plot' and 'Output' are boolen variable indicating whether the plot  and outputs are required or need to be suppressed. If Plot is TRUE, in addition
to plotting the histogram the function outputs a list of three numbers if 'Result' is TRUE -
1. the difference between theoretical and the actual mean.
2. the difference between the theoretical and actual variance.
3. The coverage in percentage.
---
title: Impact of Supplements on Tooth Growth
output: pdf_document
---
This study performs an analysis of the given data presented in 'ToothGrowth'.
ToothGrowth is  data frame with 60 observations on 3 variables.
[,1]  len  numeric	Tooth length
[,2]	supp	factor	Supplement type (VC or OJ).
[,3]	dose	numeric	Dose in milligrams.
The data is measurements of the length of odontoblasts (teeth) in each of 10 guinea pigs at each of three dose
levels of Vitamin C (0.5, 1, and 2 mg) with each of two delivery methods (orange juice or ascorbic acid).
**Exploratory Analysis**: This is a small dataset, However we might want to know what kind of values there are
in the table so we examine them using the 'str' command.
```{r echo = FALSE}
str(ToothGrowth)
```
The basic summary of the data presented is
```{r echo = FALSE}
summary(ToothGrowth)
```
data(sleep)
head(sleep)
str(sleep)
?t.test
g1 <- sleep$extra[1 : 10]; g2 <- sleep$extra[11 : 20]
t.test(g2, g1, paired = TRUE)
t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)
1100+c(-1,1)*qt(0.975,8)*10
6/qt(0.975,8)
-2+c(-1,1)*qt(0.975,18)*(0.128)^(0.5)
library(datasets); data(ChickWeight); library(reshape2)
str(chickweight)
str(chickWeight)
ls()
str(ChickWeight)
HealthVec <- Health$TotalHealthLoss
EconomyVec <- Economy$TotalEconomicLoss
names(EconomyVec) <- Economy$EventType
names(HealthVec) <- Health$EventType
Health <- head(arrange(HealthTable, desc(TotalHealthLoss)), 10)
Economy <- head(arrange(EconomyTable, desc(TotalEconomicLoss)), 10)
HealthVec <- Health$TotalHealthLoss
EconomyVec <- Economy$TotalEconomicLoss
names(EconomyVec) <- Economy$EventType
names(HealthVec) <- Health$EventType
HealthTable <- summarize(ByEventType, TotalHealthLoss = sum(HealthDamage))
EconomyTable <- summarize(ByEventType, TotalEconomicLoss = sum(EconomicDamage))
#Extracting the top 15 items in each category
Health <- head(arrange(HealthTable, desc(TotalHealthLoss)), 10)
Economy <- head(arrange(EconomyTable, desc(TotalEconomicLoss)), 10)
HealthVec <- Health$TotalHealthLoss
EconomyVec <- Economy$TotalEconomicLoss
names(EconomyVec) <- Economy$EventType
names(HealthVec) <- Health$EventType
Impact of Natural Hazards on Health and Economy of Population of USA as Reflected by Data Recorded Between 1952  to 2011
=======================================
##Synopsis:
In this study we investigate the extent of damage caused in terms of both property loss and loss of quality of life or life itself of the population of USA due to natural causes. This study is performed using, Storm Data, NOAA’s official publication which documents the occurrence of storms and other significant natural hazards having sufficient intensity to cause loss of life, injuries, significant property damage, and/or disruption to commerce. This is a precious pulic source of data but one that takes substantial effort to clean up before being used. One of the problems with using this data is the inconsistencies in terminology used.  We cleaned up the data and aggregated it into two categories - impact on property and impact on life and found that the.
conclusions.
##Data Processing
(1) **Downloading and reading data into the workspace**
This dataset in .bz2 format was downloaded from the following url:
"https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2".
I downloded the data by hand because it is a large dataset and takes time to download.
```{r cache = TRUE}
InputTable <- read.table("repdata_data_StormData.csv.bz2", header = TRUE, sep = ",")
```
(2) **Pruning data** we examine the column names of the input data and select only seven columns relevant to our study, which are: event type(EVTYPE), property damage(PROPDMG), crop damage(CRoPDMG), order of magnitude of property and crop damage(PROPDMGEXP and CROPDMGEXP), fatalities (FATALITIES) and injuries(INJURIES).
```{r message=FALSE, warning=FALSE}
#selecting the columns we need
oldCols <- colnames(InputTable)
newCols <- c(rep("NULL", 7), "EventType", rep("NULL",14), "Fatalities", "Injuries", "PropertyDamage", "PropOrder","CropDamage",  "CropOrder", rep("NULL", 9) )
library(data.table)
library(dplyr)
FilteredTable <- setnames(InputTable, oldCols, newCols)
FilteredTable <- data.table(FilteredTable)
NewTable <- FilteredTable[ , grep("NULL", colnames(FilteredTable)) := NULL]
str(NewTable)
```
(3) When we examine the table using str() command, we see that the entries in the columns are not clean and  and we need to do it to the degree possible for every column. We first consider the EventType variable; NOAA lists 48 types of events in this category and these are supposed ot be the legitimate names to be used in the report. We examined what proportion of the rows have one of these 48 events.
```{r}
#Extracting the entries with just the 48 types of storm data event types presented by NOAA.
eventList <- tolower(c("Astronomical Low Tide", "Avalanche", "Blizzard", "Coastal Flood", "Cold/Wind Chill", "Debris Flow", "Dense Fog",
"Dense Smoke", "Drought", "Dust Devil", "Dust Storm", "Excessive Heat", "Extreme Cold/Wind Chill", "Flash Flood",
"Flood", "Frost/Freeze", "Funnel Cloud", "Freezing Fog", "Hail", "Heat", "Heavy Rain", "Heavy Snow", "High Surf",
"High Wind", "Hurricane (Typhoon)", "Ice Storm", "Lake-Effect Snow", "Lakeshore Flood", "Lightning", "Marine Hail",
"Marine High Wind", "Marine Strong Wind", "Marine Thunderstorm Wind", "Rip Current", "Seiche", "Sleet", "Storm Surge/Tide",
"Strong Wind", "Thunderstorm Wind", "Tornado", "Tropical Depression", "Tropical Storm", "Tsunami", "Volcanic Ash",
"Waterspout", "Wildfire", "Winter Storm", "Winter Weather"))
NewTable$EventType <- tolower(NewTable$EventType)
InEventList <- sapply(NewTable$EventType, function(x) (x %in% eventList))
proportion <- sum(InEventList)/length(NewTable$EventType)
```
The percentage of rows with a legitimate event name is `r proportion*100` percent. Since the rest of the data is very messy and this is a large enough sample, we filter out other rows.
```{r}
NewTable <- NewTable[(NewTable$EventType %in% eventList),  ]
NewTable$EventType <- factor(NewTable$EventType)
str(NewTable$EventType)
```
Next we clean up the 'PropOrder' and 'CropOrder' columns. We assume these values are meant to be the exponent multiplicative factors to property damage and crop damage.
```{r}
NewTable$PropOrder <- tolower(NewTable$PropOrder)
NewTable$CropOrder <- tolower(NewTable$CropOrder)
Remove <- c("?", "+", "-")
NewTable <- NewTable[!(NewTable$PropOrder %in% Remove | NewTable$CropOrder %in% Remove), ]
expSubst <- function(x){
if ((x =="")||(x=="0")) ret = "1"
if (x =="1") ret = "10"
if ((x =="2")||(x=="h")) ret = "100"
if ((x =="3")||(x=="k")) ret = "1000"
if (x =="4") ret = "10000"
if (x =="5") ret = "100000"
if ((x =="6")||(x=="m")) ret = "1000000"
if (x =="7") ret = "10000000"
if (x =="8") ret = "100000000"
if ((x =="9")||(x=="b")) ret = "1000000000"
ret
}
NewVals <- sapply(NewTable$PropOrder, expSubst)
NewTable$PropOrder <- NewVals
NewVals <- sapply(NewTable$CropOrder, expSubst)
NewTable$CropOrder <- NewVals
NewTable$CropOrder <- as.numeric(NewTable$CropOrder)
NewTable$PropOrder <- as.numeric(NewTable$PropOrder)
NewTable <- NewTable[complete.cases(NewTable), ]
# Multiply PropertyDamage and CropDamage by PropOrder and CropOrder, respectively and get rid of the latter two columns
rows <- length(NewTable$Year)
NewCrop <- (NewTable$CropDamage)*(NewTable$CropOrder)
NewTable$CropDamage <- NewCrop
NewProp <- (NewTable$PropertyDamage)*(NewTable$PropOrder)
NewTable$PropertyDamage <- NewProp
```
We now make a decision that Fatalities and Injuries can both be counted as damage to the health of the population and similarly, both property damage and crop damage can be counted as economic impairments so we club these two pairs of columns and give them new names.
```{r}
NewTable <- mutate(NewTable, EconomicDamage = PropertyDamage+CropDamage)
NewTable <- mutate(NewTable, HealthDamage = Fatalities+Injuries)
#Remove redundant columns
st
```
Now the data is clean and ready for analysis.
##Data Analysis
We will analyze this data and investigate the following two questions:
(1) Which event is the most harmful with respect to population health?
(2) Which events caused the greatest economic consequences?
What we need to do is to aggregate the data and show the difference between the damages caused by different event types. We only consider top 10 events. Thi sseems like a reasonable consideration because the data taper off quite a bit.
```{r}
#Grouping and summarizing
HealthTable <- summarize(ByEventType, TotalHealthLoss = sum(HealthDamage))
EconomyTable <- summarize(ByEventType, TotalEconomicLoss = sum(EconomicDamage))
#Extracting the top 15 items in each category
Health <- head(arrange(HealthTable, desc(TotalHealthLoss)), 10)
Economy <- head(arrange(EconomyTable, desc(TotalEconomicLoss)), 10)
HealthVec <- Health$TotalHealthLoss
EconomyVec <- Economy$TotalEconomicLoss
names(EconomyVec) <- Economy$EventType
names(HealthVec) <- Health$EventType
Impact of Natural Hazards on Health and Economy of Population of USA as Reflected by Data Recorded Between 1952  to 2011
=======================================
##Synopsis:
In this study we investigate the extent of damage caused in terms of both property loss and loss of quality of life or life itself of the population of USA due to natural causes. This study is performed using, Storm Data, NOAA’s official publication which documents the occurrence of storms and other significant natural hazards having sufficient intensity to cause loss of life, injuries, significant property damage, and/or disruption to commerce. This is a precious pulic source of data but one that takes substantial effort to clean up before being used. One of the problems with using this data is the inconsistencies in terminology used.  We cleaned up the data and aggregated it into two categories - impact on property and impact on life and found that the.
conclusions.
##Data Processing
(1) **Downloading and reading data into the workspace**
This dataset in .bz2 format was downloaded from the following url:
"https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2".
I downloded the data by hand because it is a large dataset and takes time to download.
```{r cache = TRUE}
InputTable <- read.table("repdata_data_StormData.csv.bz2", header = TRUE, sep = ",")
```
(2) **Pruning data** we examine the column names of the input data and select only seven columns relevant to our study, which are: event type(EVTYPE), property damage(PROPDMG), crop damage(CRoPDMG), order of magnitude of property and crop damage(PROPDMGEXP and CROPDMGEXP), fatalities (FATALITIES) and injuries(INJURIES).
```{r message=FALSE, warning=FALSE}
#selecting the columns we need
oldCols <- colnames(InputTable)
newCols <- c(rep("NULL", 7), "EventType", rep("NULL",14), "Fatalities", "Injuries", "PropertyDamage", "PropOrder","CropDamage",  "CropOrder", rep("NULL", 9) )
library(data.table)
library(dplyr)
FilteredTable <- setnames(InputTable, oldCols, newCols)
FilteredTable <- data.table(FilteredTable)
NewTable <- FilteredTable[ , grep("NULL", colnames(FilteredTable)) := NULL]
str(NewTable)
```
(3) When we examine the table using str() command, we see that the entries in the columns are not clean and  and we need to do it to the degree possible for every column. We first consider the EventType variable; NOAA lists 48 types of events in this category and these are supposed ot be the legitimate names to be used in the report. We examined what proportion of the rows have one of these 48 events.
```{r}
#Extracting the entries with just the 48 types of storm data event types presented by NOAA.
eventList <- tolower(c("Astronomical Low Tide", "Avalanche", "Blizzard", "Coastal Flood", "Cold/Wind Chill", "Debris Flow", "Dense Fog",
"Dense Smoke", "Drought", "Dust Devil", "Dust Storm", "Excessive Heat", "Extreme Cold/Wind Chill", "Flash Flood",
"Flood", "Frost/Freeze", "Funnel Cloud", "Freezing Fog", "Hail", "Heat", "Heavy Rain", "Heavy Snow", "High Surf",
"High Wind", "Hurricane (Typhoon)", "Ice Storm", "Lake-Effect Snow", "Lakeshore Flood", "Lightning", "Marine Hail",
"Marine High Wind", "Marine Strong Wind", "Marine Thunderstorm Wind", "Rip Current", "Seiche", "Sleet", "Storm Surge/Tide",
"Strong Wind", "Thunderstorm Wind", "Tornado", "Tropical Depression", "Tropical Storm", "Tsunami", "Volcanic Ash",
"Waterspout", "Wildfire", "Winter Storm", "Winter Weather"))
NewTable$EventType <- tolower(NewTable$EventType)
InEventList <- sapply(NewTable$EventType, function(x) (x %in% eventList))
proportion <- sum(InEventList)/length(NewTable$EventType)
```
The percentage of rows with a legitimate event name is `r proportion*100` percent. Since the rest of the data is very messy and this is a large enough sample, we filter out other rows.
```{r}
NewTable <- NewTable[(NewTable$EventType %in% eventList),  ]
NewTable$EventType <- factor(NewTable$EventType)
str(NewTable$EventType)
```
Next we clean up the 'PropOrder' and 'CropOrder' columns. We assume these values are meant to be the exponent multiplicative factors to property damage and crop damage.
```{r}
NewTable$PropOrder <- tolower(NewTable$PropOrder)
NewTable$CropOrder <- tolower(NewTable$CropOrder)
Remove <- c("?", "+", "-")
NewTable <- NewTable[!(NewTable$PropOrder %in% Remove | NewTable$CropOrder %in% Remove), ]
expSubst <- function(x){
if ((x =="")||(x=="0")) ret = "1"
if (x =="1") ret = "10"
if ((x =="2")||(x=="h")) ret = "100"
if ((x =="3")||(x=="k")) ret = "1000"
if (x =="4") ret = "10000"
if (x =="5") ret = "100000"
if ((x =="6")||(x=="m")) ret = "1000000"
if (x =="7") ret = "10000000"
if (x =="8") ret = "100000000"
if ((x =="9")||(x=="b")) ret = "1000000000"
ret
}
NewVals <- sapply(NewTable$PropOrder, expSubst)
NewTable$PropOrder <- NewVals
NewVals <- sapply(NewTable$CropOrder, expSubst)
NewTable$CropOrder <- NewVals
NewTable$CropOrder <- as.numeric(NewTable$CropOrder)
NewTable$PropOrder <- as.numeric(NewTable$PropOrder)
NewTable <- NewTable[complete.cases(NewTable), ]
# Multiply PropertyDamage and CropDamage by PropOrder and CropOrder, respectively and get rid of the latter two columns
rows <- length(NewTable$Year)
NewCrop <- (NewTable$CropDamage)*(NewTable$CropOrder)
NewTable$CropDamage <- NewCrop
NewProp <- (NewTable$PropertyDamage)*(NewTable$PropOrder)
NewTable$PropertyDamage <- NewProp
```
We now make a decision that Fatalities and Injuries can both be counted as damage to the health of the population and similarly, both property damage and crop damage can be counted as economic impairments so we club these two pairs of columns and give them new names.
```{r}
NewTable <- mutate(NewTable, EconomicDamage = PropertyDamage+CropDamage)
NewTable <- mutate(NewTable, HealthDamage = Fatalities+Injuries)
#Remove redundant columns
st
```
Now the data is clean and ready for analysis.
##Data Analysis
We will analyze this data and investigate the following two questions:
(1) Which event is the most harmful with respect to population health?
(2) Which events caused the greatest economic consequences?
What we need to do is to aggregate the data and show the difference between the damages caused by different event types. We only consider top 10 events. Thi sseems like a reasonable consideration because the data taper off quite a bit.
```{r}
#Grouping and summarizing
HealthTable <- summarize(ByEventType, TotalHealthLoss = sum(HealthDamage))
EconomyTable <- summarize(ByEventType, TotalEconomicLoss = sum(EconomicDamage))
#Extracting the top 15 items in each category
Health <- head(arrange(HealthTable, desc(TotalHealthLoss)), 10)
Economy <- head(arrange(EconomyTable, desc(TotalEconomicLoss)), 10)
HealthVec <- Health$TotalHealthLoss
EconomyVec <- Economy$TotalEconomicLoss
names(EconomyVec) <- Economy$EventType
names(HealthVec) <- Health$EventType
names(EconomyVec) <- Economy$EventType
names(HealthVec) <- Health$EventType
#Plotting
barplot(HealthVec, col=rainbow(10), main = "Total Health Damage caused by the Top Ten Natural Events", las =2)
barplot(EconomyVec, col=rainbow(10), main = "Total Property Damage Caused by  of Top Ten Natural Events", las =2)
```
myPlot <- function(beta){
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
plot(
as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent",
ylab = "child"
)
abline(0, beta, lwd = 3)
points(0, 0, cex = 2, pch = 19)
mse <- mean( (y - beta * x)^2 )
title(paste("beta = ", beta, "mse = ", round(mse, 3)))
}
manipulate(myPlot(beta), beta = manipulate::slider(0.4, .8, step = 0.02))
install.packages("manipulate")
swirl()
library(swirl)
swirl()
cor(gpa_nor, gch_nor)
l_nor <- lm(gch_nor~gpa_nor)
swirl()
x <- c(0.18, -1.54, 0.42, 0.95)
z <- sum(x)/8
z
z1 <- sum(x)/2
z1
w <- c(2,1,3,1)
calc <- function(mu)sum(w*(x-mu)^2)
mus(0.1471,0.0025, 0.300, 1.077, 0.00125)
mus<- c(0.1471,0.0025, 0.300, 1.077, 0.00125)
result <- sapply(mus, calc)
result
+
0
2*sum(x0)
2*sum(x)
result <- sum(w^2*x)/sum(w^2)
result
2*sum(x*w)/sum(w)
sum(x)
x*w
2(x*w)
sum(2*(x*w))
sum(w)
2.06/7
2.06/4
2.06/14
calc(0.2942857)
require(swirl)
swirl()
require(swirl0)
require(swirl)
swirl()
require(swirl)
swirl
swirl()
require(swirl)
swirl()
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
lm(y~x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)
summary(fit)$sigma
data(mtcars)
head(mtcars)
fit2 <- lm(mpg~wt, mtcars)
avgwt <- mean(mtcars$wt)
avgwt
predict(fit2, avgwt, interval = "confidence")
predict(fit2, avgwt, interval = ("confidence"))
predict(fit2, mean(mtcars$wt), interval = ("confidence"))
fit2 <- lm(mpg~wt, mtcars)
avgwt <- mean(mtcars$wt)
predict(fit2, mean(mtcars$wt), interval = ("confidence"))
avgwt <- data.frame(mean(mtcars$wt))
predict(fit2, avgwt, interval = ("confidence"))
fit2 <- lm(mpg~wt, mtcars)
avgwt <- data.frame(mean(mtcars$wt))
predict(fit2, avgwt, interval = ("confidence"))
predict(fit2, 3.2172, interval = ("confidence"))
data(mtcars)
head(mtcars)
fit2 <- lm(mpg~wt, mtcars)
avgwt <- data.frame(mean(mtcars$wt))
predict(fit2, 3.2172, interval = ("confidence"))
data(mtcars)
head(mtcars)
fit2 <- lm(mpg~wt, mtcars)
avgwt <- data.frame(mean(mtcars$wt))
predict(fit2, avgwt, interval = ("confidence"))
fit2 <- lm(mpg~wt, mtcars)
avgwt <- mean(mtcars$wt)
avgwt1 <- data.frame(avgwt)
predict(fit2, avgwt1, interval = ("confidence"))
predict(lm(mpg~wt, mtcars), data.frame(mean(mtcars$wt)), interval = ("confidence"))
x <- runif(10)
y <- 5 + 2.7 * x + rnorm(10, mean=0, sd=sqrt(0.15))
#fit the model and store the coefficients
regLin <- lm(y~x)
coef <- coef(regLin)
coef
#use the predict() function
y_star2 <- predict(regLin)
#use equation (2)
y_star1 <- coef[1] + coef[2] * x
#compare
cbind(y, y_star1, y_star2)
y  y_star1  y_star2
# 1  7.100217 6.813616 6.813616
# 2  6.186333 5.785473 5.785473
# 3  7.141016 7.492979 7.492979
# 4  5.121265 5.282990 5.282990
# 5  4.681924 4.849776 4.849776
# 6  6.102339 6.106751 6.106751
# 7  7.223215 7.156512 7.156512
# 8  5.158546 5.253380 5.253380
# 9  7.160201 7.198074 7.198074
# 10 5.555289 5.490793 5.490793
x <- runif(10)
y <- 5 + 2.7 * x + rnorm(10, mean=0, sd=sqrt(0.15))
#fit the model and store the coefficients
regLin <- lm(y~x)
coef <- coef(regLin)
coef
#use the predict() function
y_star2 <- predict(regLin)
#use equation (2)
y_star1 <- coef[1] + coef[2] * x
#compare
cbind(y, y_star1, y_star2)
#y  y_star1  y_star2
# 1  7.100217 6.813616 6.813616
# 2  6.186333 5.785473 5.785473
# 3  7.141016 7.492979 7.492979
# 4  5.121265 5.282990 5.282990
# 5  4.681924 4.849776 4.849776
# 6  6.102339 6.106751 6.106751
# 7  7.223215 7.156512 7.156512
# 8  5.158546 5.253380 5.253380
# 9  7.160201 7.198074 7.198074
# 10 5.555289 5.490793 5.490793
x <- runif(10)
y <- 5 + 2.7 * x + rnorm(10, mean=0, sd=sqrt(0.15))
#fit the model and store the coefficients
regLin <- lm(y~x)
coef <- coef(regLin)
coef
#value of y ignoring the random term 'rnorm(...)'
y0 = 5 +2.7*x
#use the predict() function
y_star2 <- predict(regLin)
#use equation (2)
y_star1 <- coef[1] + coef[2] * x
#compare
cbind(y0, y, y_star1, y_star2)
#y  y_star1  y_star2
# 1  7.100217 6.813616 6.813616
x <-  c( 2, 5, 4, 5, 2, 8, 4, 6)
ord <- order(x)
ord
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret")
install.packages("caret")
library("bitops", lib.loc="~/R/win-library/3.1")
library("RCurl", lib.loc="~/R/win-library/3.1")
setwd("~/Coursera/courses/09_DevelopingDataProducts/QuizzAndCourseproject/SlidifyBuyDiamonds/BuyDiamondsPresentation")
